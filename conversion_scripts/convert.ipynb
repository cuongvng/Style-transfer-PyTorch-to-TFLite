{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "casual-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "from pytorch_models.model_arch import TransformerNet\n",
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "taken-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_image(filename, size=None, scale=None):\n",
    "    img = Image.open(filename).convert('RGB')\n",
    "    if size is not None:\n",
    "        img = img.resize((size, size), Image.ANTIALIAS)\n",
    "    elif scale is not None:\n",
    "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_image(filename, data):\n",
    "    img = data.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
    "    img = Image.fromarray(img)\n",
    "    img.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tracked-drama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.conv2d.weight', 'conv1.conv2d.bias', 'in1.weight', 'in1.bias', 'in1.running_mean', 'in1.running_var', 'conv2.conv2d.weight', 'conv2.conv2d.bias', 'in2.weight', 'in2.bias', 'in2.running_mean', 'in2.running_var', 'conv3.conv2d.weight', 'conv3.conv2d.bias', 'in3.weight', 'in3.bias', 'in3.running_mean', 'in3.running_var', 'res1.conv1.conv2d.weight', 'res1.conv1.conv2d.bias', 'res1.in1.weight', 'res1.in1.bias', 'res1.in1.running_mean', 'res1.in1.running_var', 'res1.conv2.conv2d.weight', 'res1.conv2.conv2d.bias', 'res1.in2.weight', 'res1.in2.bias', 'res1.in2.running_mean', 'res1.in2.running_var', 'res2.conv1.conv2d.weight', 'res2.conv1.conv2d.bias', 'res2.in1.weight', 'res2.in1.bias', 'res2.in1.running_mean', 'res2.in1.running_var', 'res2.conv2.conv2d.weight', 'res2.conv2.conv2d.bias', 'res2.in2.weight', 'res2.in2.bias', 'res2.in2.running_mean', 'res2.in2.running_var', 'res3.conv1.conv2d.weight', 'res3.conv1.conv2d.bias', 'res3.in1.weight', 'res3.in1.bias', 'res3.in1.running_mean', 'res3.in1.running_var', 'res3.conv2.conv2d.weight', 'res3.conv2.conv2d.bias', 'res3.in2.weight', 'res3.in2.bias', 'res3.in2.running_mean', 'res3.in2.running_var', 'res4.conv1.conv2d.weight', 'res4.conv1.conv2d.bias', 'res4.in1.weight', 'res4.in1.bias', 'res4.in1.running_mean', 'res4.in1.running_var', 'res4.conv2.conv2d.weight', 'res4.conv2.conv2d.bias', 'res4.in2.weight', 'res4.in2.bias', 'res4.in2.running_mean', 'res4.in2.running_var', 'res5.conv1.conv2d.weight', 'res5.conv1.conv2d.bias', 'res5.in1.weight', 'res5.in1.bias', 'res5.in1.running_mean', 'res5.in1.running_var', 'res5.conv2.conv2d.weight', 'res5.conv2.conv2d.bias', 'res5.in2.weight', 'res5.in2.bias', 'res5.in2.running_mean', 'res5.in2.running_var', 'deconv1.conv2d.weight', 'deconv1.conv2d.bias', 'in4.weight', 'in4.bias', 'in4.running_mean', 'in4.running_var', 'deconv2.conv2d.weight', 'deconv2.conv2d.bias', 'in5.weight', 'in5.bias', 'in5.running_mean', 'in5.running_var', 'deconv3.conv2d.weight', 'deconv3.conv2d.bias'])\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"./pytorch_models/rain_princess.pth\"\n",
    "state_dict = torch.load(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "furnished-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"./saved_models/rain_princess.pth\"\n",
    "state_dict = torch.load(model_checkpoint)\n",
    "\n",
    "keys_to_del = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    keys_to_del.extend([\n",
    "        f\"in{i}.running_mean\",\n",
    "        f\"in{i}.running_var\",\n",
    "    ])\n",
    "\n",
    "for i in range(1,3):\n",
    "    for r in range(1, 6):\n",
    "        keys_to_del.extend([\n",
    "            f\"res{r}.in{i}.running_mean\",\n",
    "            f\"res{r}.in{i}.running_var\"\n",
    "        ])\n",
    "\n",
    "for k in keys_to_del:\n",
    "    del state_dict[k]\n",
    "\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "modular-match",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerNet()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "single-wrapping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerNet(\n",
       "  (conv1): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "    (conv2d): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1))\n",
       "  )\n",
       "  (in1): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv2): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  )\n",
       "  (in2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (conv3): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  )\n",
       "  (in3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (res1): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res2): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res3): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res4): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (res5): ResidualBlock(\n",
       "    (conv1): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (conv2): ConvLayer(\n",
       "      (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "      (conv2d): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    )\n",
       "    (in2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (deconv1): UpsampleConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (in4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (deconv2): UpsampleConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (conv2d): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (in5): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
       "  (deconv3): ConvLayer(\n",
       "    (reflection_pad): ReflectionPad2d((4, 4, 4, 4))\n",
       "    (conv2d): Conv2d(32, 3, kernel_size=(9, 9), stride=(1, 1))\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dynamic-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some random number\n",
    "batch_size = 1\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "x = torch.randn(batch_size, 3, height, width, requires_grad=True)\n",
    "output = model(x)\n",
    "\n",
    "# Convert the PyTorch model to ONNX format\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"./temp/rain_princess.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0: \"batch_size\", 2: \"height\", 3: \"width\"},    # variable lenght axes\n",
    "                                'output' : {0: \"batch_size\",  2: \"height\", 3: \"width\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "chubby-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize_onnx(model_path, content_image_path, output_image_path):\n",
    "    \"\"\"\n",
    "    Read ONNX model and run it using onnxruntime\n",
    "    \"\"\"\n",
    "    def to_numpy(tensor):\n",
    "        if tensor.requires_grad:\n",
    "            return tensor.detach().cpu().numpy()    \n",
    "        else: \n",
    "            return tensor.cpu().numpy()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    assert model_path.endswith(\".onnx\")\n",
    "    ort_session = onnxruntime.InferenceSession(model_path)\n",
    "    \n",
    "    content_image = load_image(content_image_path, scale=None)\n",
    "    content_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    content_image = content_transform(content_image)\n",
    "    content_image = content_image.unsqueeze(0).to(device)\n",
    "\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(content_image)}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    img_out_y = torch.from_numpy(ort_outs[0])\n",
    "\n",
    "    save_image(output_image_path, img_out_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "distinguished-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "stylize_onnx(\"./temp/rain_princess.onnx\", \"./temp/test.jpg\", \"./temp/test_onnx.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-count",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style-transfer-tf-lite",
   "language": "python",
   "name": "style-transfer-tf-lite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
