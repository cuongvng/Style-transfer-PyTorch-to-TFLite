{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "third-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models.pytorch_models.model_arch import TransformerNet\n",
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "challenging-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def load_image(filename, size=None, scale=None):\n",
    "    img = Image.open(filename).convert('RGB')\n",
    "    if size is not None:\n",
    "        img = img.resize((size, size), Image.ANTIALIAS)\n",
    "    elif scale is not None:\n",
    "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
    "    return img\n",
    "\n",
    "\n",
    "def save_image(filename, data):\n",
    "    img = data.clone().clamp(0, 255).numpy()\n",
    "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
    "    img = Image.fromarray(img)\n",
    "    img.save(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"../models/pytorch_models/rain_princess.pth\"\n",
    "state_dict = torch.load(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sexual-france",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_del = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    keys_to_del.extend([\n",
    "        f\"in{i}.running_mean\",\n",
    "        f\"in{i}.running_var\",\n",
    "    ])\n",
    "\n",
    "for i in range(1,3):\n",
    "    for r in range(1, 6):\n",
    "        keys_to_del.extend([\n",
    "            f\"res{r}.in{i}.running_mean\",\n",
    "            f\"res{r}.in{i}.running_var\"\n",
    "        ])\n",
    "\n",
    "for k in keys_to_del:\n",
    "    del state_dict[k]\n",
    "\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chronic-bachelor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerNet()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-figure",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "atmospheric-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some random number\n",
    "batch_size = 1\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "x = torch.randn(batch_size, 3, height, width, requires_grad=True)\n",
    "output = model(x)\n",
    "\n",
    "# Convert the PyTorch model to ONNX format\n",
    "torch.onnx.export(model,                     # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"../models/temp/rain_princess.onnx\", # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=11,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'input' : {0: \"batch_size\", 2: \"height\", 3: \"width\"},    # variable lenght axes\n",
    "                                'output' : {0: \"batch_size\",  2: \"height\", 3: \"width\"}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "silver-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stylize_onnx(model_path, content_image_path, output_image_path):\n",
    "    \"\"\"\n",
    "    Read ONNX model and run it using onnxruntime\n",
    "    \"\"\"\n",
    "    def to_numpy(tensor):\n",
    "        if tensor.requires_grad:\n",
    "            return tensor.detach().cpu().numpy()    \n",
    "        else: \n",
    "            return tensor.cpu().numpy()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    assert model_path.endswith(\".onnx\")\n",
    "    ort_session = onnxruntime.InferenceSession(model_path)\n",
    "    \n",
    "    content_image = load_image(content_image_path, scale=None)\n",
    "    content_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    content_image = content_transform(content_image)\n",
    "    content_image = content_image.unsqueeze(0).to(device)\n",
    "\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(content_image)}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    img_out_y = torch.from_numpy(ort_outs[0])\n",
    "\n",
    "    save_image(output_image_path, img_out_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dangerous-marble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on any input images you want\n",
    "stylize_onnx(\"../models/temp/rain_princess.onnx\", \"../models/temp/test.jpg\", \"../models/temp/test_onnx.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "trained-delay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 14:52:05,129 - onnx-tf - INFO - Start converting onnx pb to tf pb:\n",
      "2021-03-08 14:52:05.170554: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-03-08 14:52:05.173682: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-03-08 14:52:13.573763: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
      "2021-03-08 14:54:06,881 - onnx-tf - INFO - Converting completes successfully.\n",
      "INFO:onnx-tf:Converting completes successfully.\n"
     ]
    }
   ],
   "source": [
    "### To TF Freeze Graph (.pb)\n",
    "!onnx-tf convert -i \"../models/temp/rain_princess.onnx\" -o  \"../models/temp/rain_princess.pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "sacred-imaging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770800"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To TF Lite format\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('../models/temp/rain_princess.pb')\n",
    "# tell converter which type of optimization techniques to use\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tf_lite_model = converter.convert()\n",
    "# save the converted model \n",
    "open('../models/tflite_models/rain_princess.tflite', 'wb').write(tf_lite_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style-transfer-tf-lite",
   "language": "python",
   "name": "style-transfer-tf-lite"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
